import runpod
import torch
import logging
import time
from transformers import AutoTokenizer, AutoModel

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# åŠ è½½ tokenizer å’Œ text_modelï¼ˆä»…æœ¬åœ°åŠ è½½ï¼‰
tokenizer = AutoTokenizer.from_pretrained(
    "/runpod-volume/hub/models--BAAI--bge-large-zh-v1.5",
    trust_remote_code=True,
    local_files_only=True
)

text_model = AutoModel.from_pretrained(
    "/runpod-volume/hub/models--BAAI--bge-large-zh-v1.5",
    trust_remote_code=True,
    local_files_only=True,
    torch_dtype=torch.float16
).cuda().eval()

# æ˜¾å¼æ¸…ç©ºåˆå§‹ç¼“å­˜
torch.cuda.empty_cache()

# æ‰“å°å½“å‰GPUä¿¡æ¯
logging.info(f"ğŸš€ å½“å‰ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")

# CUDAé¢„çƒ­
with torch.no_grad(), torch.cuda.amp.autocast():
    dummy_inputs = tokenizer(["warmup"], padding=True, return_tensors="pt", truncation=True)
    dummy_inputs = {k: v.cuda() for k, v in dummy_inputs.items()}
    _ = text_model(**dummy_inputs).last_hidden_state.mean(dim=1)
logging.info("âœ… æ–‡æœ¬æ¨¡å‹ warmup å®Œæˆ")

# æ ¸å¿ƒå¤„ç†å‡½æ•°
def handler(job):
    logging.info(f"ğŸ“¥ ä»»åŠ¡è¾“å…¥å†…å®¹:\n{job}\nğŸ“„ ç±»å‹: {type(job)}")
    try:
        # æ˜¾å¼æ¸…ç©ºCUDAç¼“å­˜
        torch.cuda.empty_cache()
        
        inputs = job["input"].get("data")
        logging.info(f"ğŸ“‹ inputså†…å®¹æ˜¯: {inputs} (ç±»å‹: {type(inputs)}, é•¿åº¦: {len(inputs) if inputs else 0})")
        if isinstance(inputs, str):
            inputs = [inputs]

        if not inputs:
            logging.warning("âš ï¸ æ•°æ®ä¸ºç©º")
            return {
                "output": {
                    "error": "Empty input provided."
                }
            }

        start_time = time.time()

        # Tokenizeré˜¶æ®µ
        encoded = tokenizer(inputs, padding=True, return_tensors="pt", truncation=True)
        encoded = {k: v.cuda() for k, v in encoded.items()}

        # æ¨ç†é˜¶æ®µ
        with torch.no_grad(), torch.cuda.amp.autocast():
            output = text_model(**encoded).last_hidden_state.mean(dim=1).cpu().tolist()

        total_time = time.time()
        logging.info(f"âœ… æ–‡æœ¬æ¨ç†å®Œæˆï¼Œç”Ÿæˆ{len(output)}ä¸ªembedding, æ¯ä¸ªç»´åº¦: {len(output[0])}ï¼Œæ€»è€—æ—¶: {total_time - start_time:.3f}s")

        return {
            "output": {
                "embeddings": output
            }
        }

    except Exception as e:
        logging.error(f"âŒ å‡ºç°å¼‚å¸¸: {str(e)}")
        torch.cuda.empty_cache()
        return {
            "output": {
                "error": str(e)
            }
        }

# å¯åŠ¨ Serverless Worker
logging.info("ğŸŸ¢ Text Worker å·²å¯åŠ¨ï¼Œç­‰å¾…ä»»åŠ¡ä¸­...")
runpod.serverless.start({"handler": handler})